{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c391666",
   "metadata": {},
   "source": [
    "# Performance Comparison of Sparse Reward Models with LunarLander-v2\n",
    "\n",
    "This notebook implements a performance comparison experiment for three models in a sparse reward environment: \n",
    "- Full Sequence Gradient Planner (エピソードの最初に1回だけ計画)\n",
    "- Proximal Policy Optimization (PPO)\n",
    "- Soft Actor-Critic (SAC)\n",
    "\n",
    "~~Standard Gradient Planner (1-step MPC方式、毎ステップ再計画)~~ は実行時間短縮のため実行しません。\n",
    "\n",
    "The experiments will be conducted in the `LunarLander-v2` environment wrapped with `SparseRewardWrapper`. We will collect data, train each model, and evaluate their performance.\n",
    "\n",
    "すべての実験パラメータは一つのセルで管理されており、簡単に変更できるようになっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# プロジェクトのルートディレクトリをPythonのパスに追加します\n",
    "# これにより、'src'ディレクトリからモジュールをインポートできるようになります\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "from tqdm.notebook import tqdm  # Jupyterノートブック用のtqdmを使用\n",
    "from src.utils.environment_wrappers import SparseRewardWrapper\n",
    "from src.agents.ppo_agent import PPOAgent\n",
    "from src.agents.sac_agent import SACAgent\n",
    "from src.agents.planner_agent import SparseRewardPlannerAgent\n",
    "# 修正した勾配プランナーをインポート\n",
    "from src.agents.gradient_planner_agent import GradientPlannerAgent\n",
    "from src.models.world_models import DynamicsModel, TerminalRewardModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "ENV_ID = 'LunarLander-v2'  # Pendulum-v1からLunarLander-v2に変更\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "env = SparseRewardWrapper(gym.make(ENV_ID))\n",
    "\n",
    "# --- ハイパーパラメータ設定 (LunarLander-v2用に調整) ---\n",
    "\n",
    "# 一般設定\n",
    "RANDOM_SEED = 42               # 乱数シード\n",
    "MODEL_FREE_STEPS = 1000000     # モデルフリー手法の学習ステップ数 (LunarLanderはより複雑なので増加)\n",
    "\n",
    "# データ収集パラメータ\n",
    "INITIAL_RANDOM_STEPS = 100000  # 初期データ収集ステップ数 (より多くのデータが必要)\n",
    "\n",
    "# 評価設定\n",
    "EVAL_EPISODES = 10             # 評価時のエピソード数\n",
    "\n",
    "# 勾配プランナーのパラメータ\n",
    "GRAD_PLAN_HORIZON = 400        # 計画ホライゾン (LunarLanderのエピソードは長いため増加)\n",
    "GRAD_NUM_ITERATIONS = 2000     # 勾配法の最適化イテレーション数 (より複雑なため増加)\n",
    "GRAD_LEARNING_RATE = 0.005     # 勾配法の学習率 (より安定した学習のため減少)\n",
    "\n",
    "# 世界モデル学習パラメータ\n",
    "WORLD_MODEL_ITERATIONS = 5     # 世界モデル学習の反復回数 (より複雑なため増加)\n",
    "WORLD_MODEL_STEPS_PER_ITER = 5000  # 各反復で収集するステップ数\n",
    "WORLD_MODEL_EPOCHS_PER_ITER = 100  # 各反復でのエポック数\n",
    "WORLD_MODEL_BATCH_SIZE = 1024      # 学習時のバッチサイズ (より大きなバッチで安定化)\n",
    "WORLD_MODEL_LR = 5e-4          # 世界モデルの学習率 (より安定した学習のため減少)\n",
    "NEW_DATA_RATIO = 0.8           # 新しく収集したデータの利用率 (0.8=80%使用、古いデータも一部保持)\n",
    "\n",
    "# モデルフリー手法の共通パラメータ\n",
    "EVAL_INTERVAL = 10000          # 評価間隔（ステップ）(より頻繁に評価)\n",
    "\n",
    "# 再現性確保のためのシード設定\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a9899",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "In this section, we will collect trajectories for each model to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect random trajectories\n",
    "def collect_random_trajectories(env, num_steps):\n",
    "    \"\"\"指定されたステップ数だけランダムに行動してデータを収集する\"\"\"\n",
    "    trajectories = []\n",
    "    episode_count = 0\n",
    "    \n",
    "    # 1エピソード分のデータを一時的に保持するリスト\n",
    "    states, actions, rewards, next_states = [], [], [], []\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    total_steps = 0\n",
    "    \n",
    "    # tqdmで進捗を表示\n",
    "    pbar = tqdm(total=num_steps, desc=\"Collecting random data\", leave=False)\n",
    "    while total_steps < num_steps:\n",
    "        action = env.action_space.sample()  # ランダムな行動をサンプリング\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append([reward]) # 報酬の次元を合わせる\n",
    "        next_states.append(next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        total_steps += 1\n",
    "        \n",
    "        # 進捗バーを更新\n",
    "        pbar.update(1)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            episode_count += 1\n",
    "            \n",
    "            # 1エピソードが終了したら、データを辞書形式にまとめてtrajectoriesに追加\n",
    "            episode_data = {\n",
    "                'states': np.array(states, dtype=np.float32),\n",
    "                'actions': np.array(actions, dtype=np.float32),\n",
    "                'rewards': np.array(rewards, dtype=np.float32),\n",
    "                'next_states': np.array(next_states, dtype=np.float32)\n",
    "            }\n",
    "            trajectories.append(episode_data)\n",
    "            \n",
    "            # 次のエピソードのためにリセット\n",
    "            states, actions, rewards, next_states = [], [], [], []\n",
    "            state, _ = env.reset()\n",
    "    \n",
    "    pbar.close()\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720d9fe",
   "metadata": {},
   "source": [
    "### Collecting Data for Each Model\n",
    "We will create instances of each agent and collect data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddab7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents\n",
    "# デバイス設定を明示的に渡す\n",
    "# LunarLander-v2は離散行動空間を持っているため、行動空間の処理を調整する必要がある\n",
    "action_dim = env.action_space.n  # 離散行動空間のサイズ (LunarLander-v2では4)\n",
    "state_dim = env.observation_space.shape[0]  # 状態空間の次元 (LunarLander-v2では8)\n",
    "\n",
    "# 連続値エージェント用のダミーaction_high ([-1, 1]の範囲に正規化)\n",
    "# 注：実際にはLunarLander-v2は離散行動空間のため、この値は内部で適切に変換される必要があります\n",
    "action_high = np.ones(action_dim)\n",
    "\n",
    "# PPOとSACエージェントを初期化\n",
    "ppo_agent = PPOAgent(\n",
    "    state_shape=env.observation_space.shape, \n",
    "    action_shape=(action_dim,),  # 離散行動空間を連続値として扱う\n",
    "    action_high=action_high, \n",
    "    device=device\n",
    ")\n",
    "sac_agent = SACAgent(\n",
    "    state_shape=env.observation_space.shape, \n",
    "    action_shape=(action_dim,),  # 離散行動空間を連続値として扱う\n",
    "    action_high=action_high, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- 行動系列全体を使用する勾配プランナー (エピソードの最初に1回だけ計画) ---\n",
    "# まずCPU上でモデルを初期化\n",
    "full_dynamics_model = DynamicsModel(state_dim=state_dim, action_dim=action_dim)\n",
    "full_reward_model = TerminalRewardModel(state_dim=state_dim, action_dim=action_dim)\n",
    "# 学習の直前にGPUに転送\n",
    "full_dynamics_model = full_dynamics_model.to(device)\n",
    "full_reward_model = full_reward_model.to(device)\n",
    "# フル行動系列を使用する勾配プランナーを設定\n",
    "full_grad_planner = GradientPlannerAgent(\n",
    "    full_dynamics_model, full_reward_model, \n",
    "    action_dim=action_dim,\n",
    "    action_high=action_high,\n",
    "    plan_horizon=GRAD_PLAN_HORIZON,\n",
    "    num_iterations=GRAD_NUM_ITERATIONS,\n",
    "    learning_rate=GRAD_LEARNING_RATE,\n",
    "    use_full_sequence=True  # エピソードの最初に1回だけ計画し、行動系列全体を保存\n",
    ")\n",
    "\n",
    "# 注意: LunarLander-v2は離散行動空間なので、連続値エージェントを使用する場合は\n",
    "# 実際に環境に送る前に行動を離散化する必要があります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActionDiscretizerWrapper - 連続値アクション→離散値アクション変換のためのラッパークラス\n",
    "class ActionDiscretizerWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, action_dim):\n",
    "        super().__init__(env)\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def action(self, action):\n",
    "        \"\"\"連続値の行動を離散値インデックスに変換\"\"\"\n",
    "        if isinstance(action, np.ndarray) and len(action.shape) > 0:\n",
    "            # 連続値ベクトルから最大値のインデックスを選択\n",
    "            # 例: [0.1, 0.7, 0.2, 0.0] → 1 (最大値0.7のインデックス)\n",
    "            return np.argmax(action)\n",
    "        return int(action)  # 単一の値の場合、整数に変換\n",
    "\n",
    "# 環境をラップしてアクション変換を可能に\n",
    "env = ActionDiscretizerWrapper(env, action_dim)\n",
    "\n",
    "# Collect trajectories for the planner model using random actions\n",
    "# 最初のシードデータとしてINITIAL_RANDOM_STEPSステップのデータを収集\n",
    "print(f\"ランダム行動で{INITIAL_RANDOM_STEPS}ステップの初期データを収集中...\")\n",
    "planner_trajectories = collect_random_trajectories(env, INITIAL_RANDOM_STEPS) \n",
    "\n",
    "# Save the collected data\n",
    "# Note: You might need to create the 'data' directory first.\n",
    "import os\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "with open('../data/random_trajectories_lunarlander.pkl', 'wb') as f:\n",
    "    pickle.dump(planner_trajectories, f)\n",
    "\n",
    "print(f\"\\n{len(planner_trajectories)} episodes of random data saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb7b54",
   "metadata": {},
   "source": [
    "## Training the Models\n",
    "Now we will train each model according to the experiment plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Helper function to collect data using the planner ---\n",
    "def collect_planner_trajectories(env, agent, num_steps):\n",
    "    \"\"\"指定されたステップ数だけプランナーを使ってデータを収集する\"\"\"\n",
    "    trajectories = []\n",
    "    episode_count = 0\n",
    "    \n",
    "    states, actions, rewards, next_states = [], [], [], []\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    total_steps = 0\n",
    "    \n",
    "    # use_full_sequence モードを使用している場合はエピソードの始めにリセット\n",
    "    if hasattr(agent, 'reset'):\n",
    "        agent.reset()  # エピソードの始めであることを通知\n",
    "    \n",
    "    # tqdmで進捗を表示\n",
    "    pbar = tqdm(total=num_steps, desc=\"Collecting data\", leave=False)\n",
    "    while total_steps < num_steps:\n",
    "        # プランナーが行動を計画\n",
    "        action = agent.exploit(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append([reward])\n",
    "        next_states.append(next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        total_steps += 1\n",
    "        \n",
    "        # 進捗バーを更新\n",
    "        pbar.update(1)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            episode_count += 1\n",
    "            episode_data = {\n",
    "                'states': np.array(states, dtype=np.float32),\n",
    "                'actions': np.array(actions, dtype=np.float32),\n",
    "                'rewards': np.array(rewards, dtype=np.float32),\n",
    "                'next_states': np.array(next_states, dtype=np.float32)\n",
    "            }\n",
    "            trajectories.append(episode_data)\n",
    "            \n",
    "            # 新しいエピソードの準備\n",
    "            states, actions, rewards, next_states = [], [], [], []\n",
    "            state, _ = env.reset()\n",
    "            \n",
    "            # use_full_sequence モードを使用している場合は新しいエピソードの始めにリセット\n",
    "            if hasattr(agent, 'reset'):\n",
    "                agent.reset()\n",
    "    \n",
    "    pbar.close()\n",
    "    return trajectories\n",
    "\n",
    "# --- 1. Planner (World Models) Iterative Training ---\n",
    "def train_world_models_iteratively(planner_agent, env, initial_trajectories, num_iterations=10, steps_per_iteration=1000, epochs_per_iteration=1, batch_size=64, learning_rate=1e-3, initial_random_steps=INITIAL_RANDOM_STEPS, new_data_ratio=1.0):\n",
    "    # print削除、tqdmでの進捗表示に一元化\n",
    "    dynamics_model = planner_agent.dynamics_model\n",
    "    reward_model = planner_agent.reward_model\n",
    "    \n",
    "    # オプティマイザ\n",
    "    dynamics_optimizer = torch.optim.Adam(dynamics_model.parameters(), lr=learning_rate)\n",
    "    reward_optimizer = torch.optim.Adam(reward_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 集約データセット\n",
    "    aggregated_trajectories = initial_trajectories\n",
    "    \n",
    "    # 学習曲線追跡用の辞書\n",
    "    learning_curve = {}\n",
    "    \n",
    "    # 初期のランダムデータ収集ステップ数を反映\n",
    "    total_steps = initial_random_steps\n",
    "    \n",
    "    # 初期評価を省略し、直接学習を開始\n",
    "    \n",
    "    # tqdmでイテレーションの進捗を表示\n",
    "    for iteration in tqdm(range(num_iterations), desc=\"World Model Training\", leave=False):\n",
    "        # --- Step 1: データセットの準備 ---\n",
    "        all_states, all_actions, all_rewards_data, all_next_states = [], [], [], []\n",
    "        for episode in aggregated_trajectories:\n",
    "            final_reward = np.sum(episode['rewards'])\n",
    "            all_states.append(torch.tensor(episode['states'], dtype=torch.float))\n",
    "            all_actions.append(torch.tensor(episode['actions'], dtype=torch.float))\n",
    "            all_next_states.append(torch.tensor(episode['next_states'], dtype=torch.float))\n",
    "            all_rewards_data.append({\n",
    "                'states': torch.tensor(episode['states'], dtype=torch.float).unsqueeze(0),\n",
    "                'actions': torch.tensor(episode['actions'], dtype=torch.float).unsqueeze(0),\n",
    "                'terminal_reward': torch.tensor([final_reward], dtype=torch.float)\n",
    "            })\n",
    "\n",
    "        # CPU上でテンソルを結合する（GPUへの転送は行わない）\n",
    "        states_tensor = torch.cat(all_states, dim=0)\n",
    "        actions_tensor = torch.cat(all_actions, dim=0)\n",
    "        next_states_tensor = torch.cat(all_next_states, dim=0)\n",
    "        \n",
    "        # CPU上のテンソルを使用してデータセットを作成\n",
    "        dynamics_dataset = TensorDataset(states_tensor, actions_tensor, next_states_tensor)\n",
    "        dynamics_loader = DataLoader(dynamics_dataset, \n",
    "                                     batch_size=batch_size, \n",
    "                                     num_workers=6,\n",
    "                                     pin_memory=True,\n",
    "                                     shuffle=True)\n",
    "        \n",
    "        # 報酬モデル用のデータを準備（エポックのループの外で1回だけ）\n",
    "        states_batch = []\n",
    "        actions_batch = []\n",
    "        rewards_batch = []\n",
    "        \n",
    "        # データをバッチ処理用に準備\n",
    "        for reward_data in all_rewards_data:\n",
    "            states_batch.append(reward_data['states'])\n",
    "            actions_batch.append(reward_data['actions'])\n",
    "            rewards_batch.append(reward_data['terminal_reward'])\n",
    "            \n",
    "        # テンソルに変換\n",
    "        states_batch = torch.cat(states_batch, dim=0)\n",
    "        actions_batch = torch.cat(actions_batch, dim=0)\n",
    "        rewards_batch = torch.cat(rewards_batch, dim=0)\n",
    "        \n",
    "        # DataLoaderの作成（エポックのループの外で1回だけ）\n",
    "        reward_dataset = TensorDataset(states_batch, actions_batch, rewards_batch)\n",
    "        reward_loader = DataLoader(reward_dataset, \n",
    "                                  batch_size=min(32, len(reward_dataset)), \n",
    "                                  num_workers=6,\n",
    "                                  shuffle=True,\n",
    "                                  pin_memory=True)\n",
    "        \n",
    "        # --- Step 2: モデルの学習 ---\n",
    "        epoch_iterator = tqdm(range(epochs_per_iteration), desc=f\"Training Epochs\", leave=False)\n",
    "        for epoch in epoch_iterator:\n",
    "            # DynamicsModelの学習\n",
    "            dynamics_model.train()\n",
    "            total_dyn_loss = 0\n",
    "            progress_bar = tqdm(dynamics_loader, desc=f\"Epoch {epoch+1}/{epochs_per_iteration}\", leave=False)\n",
    "            for s, a, next_s in progress_bar:\n",
    "                # データをここでGPUに転送\n",
    "                s = s.to(device)\n",
    "                a = a.to(device)\n",
    "                next_s = next_s.to(device)\n",
    "                \n",
    "                s_in, a_in = s.unsqueeze(1), a.unsqueeze(1)\n",
    "                pred_next_s = dynamics_model(s_in, a_in).squeeze(1)\n",
    "                loss = torch.nn.functional.mse_loss(pred_next_s, next_s)\n",
    "                dynamics_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                dynamics_optimizer.step()\n",
    "                total_dyn_loss += loss.item()\n",
    "            \n",
    "            # TerminalRewardModelの学習\n",
    "            reward_model.train()\n",
    "            total_rew_loss = 0\n",
    "            \n",
    "            # バッチ処理\n",
    "            reward_progress_bar = tqdm(reward_loader, desc=f\"Reward Model\", leave=False)\n",
    "            for s_seq, a_seq, target_rew in reward_progress_bar:\n",
    "                # データをGPUに転送\n",
    "                s_seq = s_seq.to(device)\n",
    "                a_seq = a_seq.to(device)\n",
    "                target_rew = target_rew.to(device)\n",
    "                \n",
    "                pred_rew = reward_model(s_seq, a_seq)\n",
    "                loss = torch.nn.functional.mse_loss(pred_rew.squeeze(), target_rew.squeeze())\n",
    "                reward_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                reward_optimizer.step()\n",
    "                total_rew_loss += loss.item() * s_seq.size(0)  # バッチサイズを考慮\n",
    "                \n",
    "        # 各エポックの最後にtqdmを更新\n",
    "        epoch_iterator.update(1)\n",
    "\n",
    "        # --- Step 3: 学習済みモデルで新たなデータを収集・集約 ---\n",
    "        if iteration < num_iterations - 1: # 最後のイテレーションではデータ収集は不要\n",
    "            # tqdmはcollect_planner_trajectories内で処理\n",
    "            new_trajectories = collect_planner_trajectories(env, planner_agent, steps_per_iteration)\n",
    "            \n",
    "            # 新しいデータ利用率に基づいてデータを集約\n",
    "            if new_data_ratio >= 1.0:\n",
    "                # 新しいデータを全て追加\n",
    "                aggregated_trajectories.extend(new_trajectories)\n",
    "            elif new_data_ratio <= 0:\n",
    "                # 新しいデータを使用しない（既存のデータのみ使用）\n",
    "                pass\n",
    "            else:\n",
    "                # 新しいデータをサンプリングして追加\n",
    "                num_new_to_use = int(len(new_trajectories) * new_data_ratio)\n",
    "                selected_new = random.sample(new_trajectories, num_new_to_use)\n",
    "                aggregated_trajectories.extend(selected_new)\n",
    "            \n",
    "            # この時点でのモデルの評価を記録\n",
    "            total_steps += steps_per_iteration\n",
    "            current_reward = evaluate_model(planner_agent, env, EVAL_EPISODES)\n",
    "            learning_curve[total_steps] = current_reward\n",
    "    \n",
    "    # 最終評価\n",
    "    final_reward = evaluate_model(planner_agent, env, EVAL_EPISODES)\n",
    "    if total_steps in learning_curve:\n",
    "        learning_curve[total_steps] = final_reward\n",
    "    else:\n",
    "        learning_curve[total_steps + steps_per_iteration] = final_reward\n",
    "    \n",
    "    return learning_curve\n",
    "\n",
    "# --- Model-Free (PPO/SAC) Training ---\n",
    "def train_model_free_agent(agent, env, total_steps, eval_interval=5000):\n",
    "    learning_curve = {}\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    # tqdmで進捗を表示\n",
    "    progress_bar = tqdm(range(total_steps), desc=f\"Training {agent.__class__.__name__}\", leave=False)\n",
    "    for step in progress_bar:\n",
    "        log_pi = 0.0\n",
    "        if isinstance(agent, PPOAgent):\n",
    "            action, log_pi = agent.explore(state)\n",
    "        else:\n",
    "            action = agent.explore(state)\n",
    "            \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        agent.buffer.append(state, action, reward, done, log_pi, next_state)\n",
    "\n",
    "        if isinstance(agent, PPOAgent):\n",
    "            if agent.buffer.is_full():\n",
    "                agent.update()\n",
    "        elif isinstance(agent, SACAgent):\n",
    "            agent.update()\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "        if (step + 1) % eval_interval == 0:\n",
    "            avg_reward = evaluate_model(agent, env, EVAL_EPISODES)\n",
    "            learning_curve[step + 1] = avg_reward\n",
    "            \n",
    "    return learning_curve\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_model(agent, env, num_episodes=10):\n",
    "    total_reward = 0\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # tqdmを使用してエピソードの進捗を表示\n",
    "    with tqdm(range(num_episodes), desc=\"Evaluating\", leave=False) as pbar:\n",
    "        for episode in pbar:\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            # エージェントがresetメソッドを持っていれば呼び出し\n",
    "            if hasattr(agent, 'reset'):\n",
    "                agent.reset()\n",
    "            \n",
    "            # エピソード実行\n",
    "            while not done:\n",
    "                action = agent.exploit(state)\n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            total_reward += episode_reward\n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            # 進捗バーを更新\n",
    "            pbar.update(1)\n",
    "    \n",
    "    avg_reward = total_reward / num_episodes\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdd11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execute Training and Evaluation ---\n",
    "\n",
    "# 2. Train and evaluate the Full Sequence Gradient Planner (1回だけ計画)\n",
    "# Load pre-collected random data\n",
    "with open('../data/random_trajectories_lunarlander.pkl', 'rb') as f:\n",
    "    initial_full_trajectories = pickle.load(f)\n",
    "    \n",
    "# 学習曲線データを取得するように変更\n",
    "full_grad_learning_curve = train_world_models_iteratively(\n",
    "    planner_agent=full_grad_planner,\n",
    "    env=env,\n",
    "    initial_trajectories=initial_full_trajectories,\n",
    "    num_iterations=WORLD_MODEL_ITERATIONS,\n",
    "    steps_per_iteration=WORLD_MODEL_STEPS_PER_ITER,\n",
    "    epochs_per_iteration=WORLD_MODEL_EPOCHS_PER_ITER,\n",
    "    batch_size=WORLD_MODEL_BATCH_SIZE,\n",
    "    learning_rate=WORLD_MODEL_LR,\n",
    "    initial_random_steps=INITIAL_RANDOM_STEPS,\n",
    "    new_data_ratio=NEW_DATA_RATIO\n",
    ")\n",
    "# 最終評価報酬を取得（プロット用）\n",
    "full_grad_reward = list(full_grad_learning_curve.values())[-1]\n",
    "\n",
    "# 3. Train and evaluate SAC\n",
    "sac_agent_train = SACAgent(\n",
    "    state_shape=env.observation_space.shape, \n",
    "    action_shape=(action_dim,),\n",
    "    action_high=action_high, \n",
    "    device=device\n",
    ")\n",
    "sac_learning_curve = train_model_free_agent(sac_agent_train, env, MODEL_FREE_STEPS, eval_interval=EVAL_INTERVAL)\n",
    "\n",
    "# 4. Train and evaluate PPO\n",
    "ppo_learning_curve = train_model_free_agent(ppo_agent, env, MODEL_FREE_STEPS, eval_interval=EVAL_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b5da2",
   "metadata": {},
   "source": [
    "## Results\n",
    "Finally, we will plot the results of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot PPO learning curve\n",
    "if 'ppo_learning_curve' in locals() and ppo_learning_curve:\n",
    "    ppo_steps = list(ppo_learning_curve.keys())\n",
    "    ppo_rewards = list(ppo_learning_curve.values())\n",
    "    plt.plot(ppo_steps, ppo_rewards, label='PPO', marker='o')\n",
    "\n",
    "# Plot SAC learning curve\n",
    "if 'sac_learning_curve' in locals() and sac_learning_curve:\n",
    "    sac_steps = list(sac_learning_curve.keys())\n",
    "    sac_rewards = list(sac_learning_curve.values())\n",
    "    plt.plot(sac_steps, sac_rewards, label='SAC', marker='s')\n",
    "\n",
    "# Plot Full Sequence Gradient Planner's learning curve as a step function\n",
    "if 'full_grad_learning_curve' in locals() and full_grad_learning_curve:\n",
    "    # キーが昇順にソートされていることを確認\n",
    "    full_steps = sorted(list(full_grad_learning_curve.keys()))\n",
    "    full_rewards = [full_grad_learning_curve[step] for step in full_steps]\n",
    "    \n",
    "    # 値がある場合のみグラフを描画\n",
    "    if full_rewards:\n",
    "        # 階段状のグラフを描画\n",
    "        plt.step(full_steps, full_rewards, where='post', color='r', linestyle='-', \n",
    "                marker='+', markersize=8, label=f'Full Seq Gradient Planner (1回計画, Final: {full_rewards[-1]:.2f})')\n",
    "        \n",
    "        # 各ポイントをプロット\n",
    "        plt.scatter(full_steps, full_rewards, color='r', s=50)\n",
    "\n",
    "plt.xlabel('Total Environment Steps')\n",
    "plt.ylabel('Average Evaluation Reward')\n",
    "plt.title('LunarLander-v2における手法の比較')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各モデルの最適方策を使って動画を記録・表示する\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import glob\n",
    "import os\n",
    "from IPython import display\n",
    "import base64\n",
    "\n",
    "# 動画を表示するヘルパー関数\n",
    "def show_video(video_path):\n",
    "    \"\"\"\n",
    "    指定したパスの動画をノートブック内に表示する\n",
    "    \"\"\"\n",
    "    video_file = open(video_path, \"r+b\").read()\n",
    "    video_url = f\"data:video/mp4;base64,{base64.b64encode(video_file).decode()}\"\n",
    "    return HTML(f\"\"\"<video width=600 controls>\n",
    "                    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "                    </video>\"\"\")\n",
    "\n",
    "# 動画を記録するためのディレクトリを作成\n",
    "video_dir = \"../videos/lunarlander\"\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# 各モデルについて最終的な方策を実行して動画を記録\n",
    "models = {\n",
    "    \"Full_Sequence_Planner\": full_grad_planner,\n",
    "    \"PPO\": ppo_agent,\n",
    "    \"SAC\": sac_agent_train\n",
    "}\n",
    "\n",
    "# 各モデルの評価と動画記録\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- {model_name} の動画記録 ---\")\n",
    "    \n",
    "    # 動画記録用のラッパー環境を作成 (render_mode=\"rgb_array\"を指定)\n",
    "    video_env = RecordVideo(\n",
    "        ActionDiscretizerWrapper(\n",
    "            SparseRewardWrapper(gym.make(ENV_ID, render_mode=\"rgb_array\")),\n",
    "            action_dim\n",
    "        ),\n",
    "        video_folder=f\"{video_dir}/{model_name}\",\n",
    "        name_prefix=f\"{model_name}_episode\",\n",
    "        episode_trigger=lambda x: True  # すべてのエピソードを記録\n",
    "    )\n",
    "    \n",
    "    # エピソード実行\n",
    "    state, _ = video_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    # エージェントがresetメソッドを持っていれば呼び出し\n",
    "    if hasattr(model, 'reset'):\n",
    "        model.reset()\n",
    "    \n",
    "    # エピソード実行\n",
    "    while not done:\n",
    "        action = model.exploit(state)  # 最適方策での行動\n",
    "        state, reward, terminated, truncated, _ = video_env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    video_env.close()\n",
    "    \n",
    "    print(f\"{model_name} の総報酬: {total_reward:.2f}\")\n",
    "    \n",
    "    # 最新の動画ファイルを取得して表示\n",
    "    video_files = sorted(glob.glob(f\"{video_dir}/{model_name}/*.mp4\"))\n",
    "    if video_files:\n",
    "        latest_video = video_files[-1]\n",
    "        print(f\"動画ファイル: {latest_video}\")\n",
    "        display.display(show_video(latest_video))\n",
    "    else:\n",
    "        print(f\"警告: {model_name} の動画ファイルが見つかりませんでした。\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
